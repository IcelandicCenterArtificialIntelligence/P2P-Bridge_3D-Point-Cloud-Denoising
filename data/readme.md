### ScanNet++
#### Preparation
First, download the dataset from the official [Website](https://kaldir.vc.in.tum.de/scannetpp/).
Once downloaded, follow the steps below to prepare the data. This will generate a `iphone.ply` file for each scene in the dataset. This file contains a noisy point cloud generated by projecting the RGBD frames, using the provided refined poses using COLMAP and depth-maps prefiltered according to their agreement with the Faro scan. Note that the script will take a while to run and the ScanNet++ can grow up to 1TB. We hereafter refer to the root directory of the ScanNet++ dataset's `data` folder as `SNPP_ROOT`.

```bash
cd data/scannetpp
python -m iphone.process_dataset --data_root <SNPP_ROOT>
```

#### Extracting Features
To extract DINOV2 features, run the following command from the `data` directory. If you have a GPU with at least 16GB of memory, you can set `--nprocs` to a higher value for parallel processing. On a 24GB GPU, we were able to set `--nprocs` to 4. If your GPU has less memory, you might additionally set the batch size `--batch_size` to a lower value. The default value is 8.

```bash
python extract_image_features_snpp.py --data_root <SNPP_ROOT> --nprocs 1
```

#### Create Training Batches
To align the clean data with the noisy data and extract spherical training batches, run the following command from the `data` directory. It also supports the `--nprocs` flag for parallel processing. The default value is 4.

```bash
python preprocess_batches.py --data_root <SNPP_ROOT> --output_root <OUTPUT ROOT> --nprocs 1
```

### ARKitScenes
As this dataset is extremely big (over 10TB) we skip this step. If there is interest in the preprocessing step, please open an issue.